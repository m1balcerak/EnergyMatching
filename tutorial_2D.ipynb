{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Energy Matching: 2D Tutorial\n\nThis notebook demonstrates training a scalar energy potential on a 2D dataset, mapping from an \"8 Gaussians\" distribution to a \"Two Moons\" target. The trained model is then used to perform:\n\n- (a) **Unconditional sample generation**\n- (b) **Conditional posterior sampling**\n- (c) **Conditional posterior sampling with additional interaction energies**"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "import os\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nclass PotentialModel(nn.Module):\n    def __init__(self, dim=2, w=128, time_varying=True):\n        super().__init__()\n        self.time_varying = time_varying\n        self.net = nn.Sequential(\n            nn.Linear(dim + (1 if time_varying else 0), w),\n            nn.ReLU(),\n            nn.Linear(w, w),\n            nn.SiLU(),\n            nn.Linear(w, w),\n            nn.SiLU(),\n            nn.Linear(w, w),\n            nn.SiLU(),\n            nn.Linear(w, 1)\n        )\n\n    def forward(self, x, t=None):\n        if not self.time_varying:\n            return self.net(x)\n        if t is None:\n            raise ValueError('time_varying=True but t is None.')\n        if t.dim() == 0:\n            t = t.expand(x.size(0)).unsqueeze(-1)\n        elif t.dim() == 1:\n            if t.size(0) != x.size(0):\n                t = t.expand(x.size(0)).unsqueeze(-1)\n            else:\n                t = t.unsqueeze(-1)\n        t_clamped = torch.clamp(t, max=0.0)\n        inp = torch.cat([x, t_clamped], dim=-1)\n        return self.net(inp)\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "from utils_2D import train\n\n# Reproducibility\nSEED = 42\nos.environ['PYTHONHASHSEED'] = str(SEED)\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Hyperparameters\nepochs_phase1 = 200\nepochs_phase2 = 0\nbatch_size = 256\nlr = 1e-4\nflow_loss_weight = 1.0\nebm_loss_weight = 1.0\nsigma = 0.1\nsave_dir = \"2D_toy\"\n\nmodel = train(PotentialModel,\n              device=device,\n              batch_size=batch_size,\n              lr=lr,\n              epochs_phase1=epochs_phase1,\n              epochs_phase2=epochs_phase2,\n              flow_weight=flow_loss_weight,\n              ebm_weight=ebm_loss_weight,\n              sigma=sigma,\n              save_dir=save_dir)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "\nThe following Langevin sampling algorithm is used in this notebook. In case (a) we consider only the potential energy $V_{\\theta}(x)$ without additional terms.\n\n\\begin{algorithm}[H]\n\\small\n\\caption{Sampling for inverse problems (with optional interaction energy)}\n\\label{alg:sampling_inverse_interaction}\n\\begin{algorithmic}[1]\n\\For{\\(m = 1\\) to \\(M\\)}\n  \\State Initialize $x_m^{(0)} \\sim \\mathcal{N}(0, I)$ \\Comment{Start each chain from Gaussian prior}\n\\EndFor\n\\State $N \\gets \\lfloor \\samplingTime / \\Delta t \\rfloor$ \\Comment{Number of Langevin steps for sampling time $\\samplingTime$}\n\\For{\\(n = 0\\) to \\(N - 1\\)}\n  \\For{\\(m = 1\\) to \\(M\\)} \\Comment{Data fidelity + prior + interaction energy}\n    \\State $U_\\theta(x_m^{(n)}) \\gets \\frac{\\varepsilon^{(n)}}{\\zeta^2}\\lVert y - A(x_m^{(n)})\\rVert^2 + V_\\theta(x_m^{(n)}) - \\frac{\\varepsilon^{(n)}}{\\sigma^2} \\sum_{k \\neq m} W(x_m^{(n)}, x_k^{(n)})$\n    \\State $\\eta \\sim \\mathcal{N}(0, I)$ \\Comment{Gaussian noise for Langevin step}\n    \\State $x_m^{(n+1)} \\gets x_m^{(n)} - \\Delta t\\,\\nabla_x U_\\theta(x_m^{(n)}) + \\sqrt{2\\varepsilon^{(n)}\\Delta t}\\;\\eta$ \\Comment{Langevin step}\n  \\EndFor\n\\EndFor\n\\end{algorithmic}\n\\end{algorithm}\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "from utils_2D import simulate_piecewise_length, plot_trajectories_custom\nfrom torchcfm.utils import sample_8gaussians\n\nx_init = sample_8gaussians(1024).to(next(model.parameters()).device)\ntraj_np, times_np = simulate_piecewise_length(model, x_init, dt=0.01, max_length=400)\nplot_trajectories_custom(traj_np)\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "energy-matching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
