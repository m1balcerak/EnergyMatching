{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# Energy Matching: 2D Tutorial\n\nThis notebook demonstrates training a scalar energy potential on a 2D dataset, mapping from an \"8 Gaussians\" distribution to a \"Two Moons\" target. The trained model is then used to perform:\n\n- (a) **Unconditional sample generation**\n- (b) **Conditional posterior sampling**\n- (c) **Conditional posterior sampling with additional interaction energies**"}, {"cell_type": "markdown", "metadata": {}, "source": "Define the neural potential network used for energy modeling."}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "import os\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nclass PotentialModel(nn.Module):\n    def __init__(self, dim=2, w=128, time_varying=True):\n        super().__init__()\n        self.time_varying = time_varying\n        self.net = nn.Sequential(\n            nn.Linear(dim + (1 if time_varying else 0), w),\n            nn.ReLU(),\n            nn.Linear(w, w),\n            nn.SiLU(),\n            nn.Linear(w, w),\n            nn.SiLU(),\n            nn.Linear(w, w),\n            nn.SiLU(),\n            nn.Linear(w, 1)\n        )\n\n    def forward(self, x, t=None):\n        if not self.time_varying:\n            return self.net(x)\n        if t is None:\n            raise ValueError('time_varying=True but t is None.')\n        if t.dim() == 0:\n            t = t.expand(x.size(0)).unsqueeze(-1)\n        elif t.dim() == 1:\n            if t.size(0) != x.size(0):\n                t = t.expand(x.size(0)).unsqueeze(-1)\n            else:\n                t = t.unsqueeze(-1)\n        t_clamped = torch.clamp(t, max=0.0)\n        inp = torch.cat([x, t_clamped], dim=-1)\n        return self.net(inp)\n"}, {"cell_type": "markdown", "metadata": {}, "source": "Set hyperparameters and train the potential function."}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "from utils_2D import train\n\n# Reproducibility\nSEED = 42\nos.environ['PYTHONHASHSEED'] = str(SEED)\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Hyperparameters\nepochs_phase1 = 200\nepochs_phase2 = 0\nbatch_size = 256\nlr = 1e-4\nflow_loss_weight = 1.0\nebm_loss_weight = 1.0\nsigma = 0.1\nsave_dir = \"2D_toy\"\n\nmodel = train(PotentialModel,\n              device=device,\n              batch_size=batch_size,\n              lr=lr,\n              epochs_phase1=epochs_phase1,\n              epochs_phase2=epochs_phase2,\n              flow_weight=flow_loss_weight,\n              ebm_weight=ebm_loss_weight,\n              sigma=sigma,\n              save_dir=save_dir)\n"}, {"cell_type": "markdown", "metadata": {}, "source": "The Langevin update used in this notebook (case **a**) only involves the potential energy $V_{\\theta}$. At step $n$ the state $x_m$ of the $m$-th chain is updated as\n\\[\n    x_m^{(n+1)} = x_m^{(n)} - \\Delta t\\, \\nabla_x V_{\\theta}(x_m^{(n)}) + \\sqrt{2 \\, \\varepsilon^{(n)} \\Delta t}\\, \\eta^{(n)}, \\quad \\eta^{(n)} \\sim \\mathcal{N}(0, I).\n\\]\nFor cases **b** and **c** additional terms will be included in $U_{\\theta}$."}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "from utils_2D import simulate_piecewise_length, plot_trajectories_custom\nfrom torchcfm.utils import sample_8gaussians\n\nx_init = sample_8gaussians(1024).to(next(model.parameters()).device)\ntraj_np, times_np = simulate_piecewise_length(model, x_init, dt=0.01, max_length=400)\nplot_trajectories_custom(traj_np)\n"}], "metadata": {"kernelspec": {"display_name": "energy-matching", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10"}}, "nbformat": 4, "nbformat_minor": 2}
